{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import minitorch.autodiff.tensor_functions as tf\n",
    "from minitorch import operators\n",
    "from minitorch.module import LinearTensorLayer, Parameter\n",
    "from minitorch.autodiff import Context, Tensor, topological_sort\n",
    "from minitorch.autodiff.tensor_ops import SimpleBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[]\n",
      "[None]\n",
      "None\n",
      "[None]\n",
      "[None, None]\n",
      "None\n",
      "[None, None]\n",
      "[None, None, None]\n",
      "None\n",
      "[None, None, None]\n",
      "[None, None, None, None]\n",
      "None\n",
      "[None, None, None, None]\n",
      "[None, None, None, None, None]\n",
      "None\n",
      "[None, None, None, None, None]\n",
      "[None, None, None, None, None, None]\n",
      "None\n",
      "[None, None, None, None, None, None]\n",
      "[None, None, None, None, None, None, None]\n",
      "None\n",
      "[None, None, None, None, None, None, None]\n",
      "[None, None, None, None, None, None, None, None]\n",
      "None\n",
      "[None, None, None, None, None, None, None, None]\n",
      "[None, None, None, None, None, None, None, None, None]\n",
      "None\n",
      "[None, None, None, None, None, None, None, None, None]\n",
      "have seen None before\n",
      "removing None\n",
      "ids ....\n",
      "[None, None, None, None, None, None, None, None, None]\n",
      "None\n",
      "[None, None, None, None, None, None, None, None, None]\n",
      "[None, None, None, None, None, None, None, None, None, None]\n",
      "None\n",
      "[None, None, None, None, None, None, None, None, None, None]\n",
      "have seen None before\n",
      "removing None\n",
      "ids ....\n",
      "[None, None, None, None, None, None, None, None, None, None]\n",
      "None\n",
      "[None, None, None, None, None, None, None, None, None, None]\n",
      "[None, None, None, None, None, None, None, None, None, None, None]\n",
      "None\n",
      "[None, None, None, None, None, None, None, None, None, None, None]\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "None\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "have seen None before\n",
      "removing None\n",
      "ids ....\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "None\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "None\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "have seen None before\n",
      "removing None\n",
      "ids ....\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "None\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "have seen None before\n",
      "removing None\n",
      "ids ....\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "None\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "None\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "have seen None before\n",
      "removing None\n",
      "ids ....\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "None\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "None\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "have seen None before\n",
      "removing None\n",
      "ids ....\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "None\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "None\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "have seen None before\n",
      "removing None\n",
      "ids ....\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "None\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "have seen None before\n",
      "removing None\n",
      "ids ....\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "running backprop on var None\n",
      "getting upward diff None\n",
      "\n",
      "[1.00000]\n",
      "calling Sum backward\n",
      "with grad out \n",
      "[1.00000]\n",
      "in chain_rule (\n",
      "[1.00000], \n",
      "[0.00000])\n",
      "setting diff for var None\n",
      "previous diff 0.0\n",
      "diff \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "new diff \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "======\n",
      "running backprop on var None\n",
      "getting upward diff dNone wrt. dNone\n",
      "\n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "calling View backward\n",
      "with grad out \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "in chain_rule (\n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000], \n",
      "[0.00000])\n",
      "setting diff for var None\n",
      "previous diff 0.0\n",
      "diff \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "new diff \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "======\n",
      "running backprop on var None\n",
      "getting upward diff dNone wrt. dNone\n",
      "\n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "calling Copy backward\n",
      "with grad out \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "in chain_rule (\n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000],)\n",
      "setting diff for var None\n",
      "previous diff 0.0\n",
      "diff \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "new diff \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "======\n",
      "running backprop on var None\n",
      "getting upward diff dNone wrt. dNone\n",
      "\n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "calling Add backward\n",
      "with grad out \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "in chain_rule (\n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000], \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000])\n",
      "setting diff for var None\n",
      "previous diff 0.0\n",
      "diff \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "new diff \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "setting diff for var None\n",
      "previous diff 0.0\n",
      "diff \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "new diff \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "======\n",
      "running backprop on var None\n",
      "getting upward diff dNone wrt. dNone\n",
      "\n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "calling Mul backward\n",
      "with grad out \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "got these saved tensors (\n",
      "[0.51903 0.49080 0.55094 0.47147 0.45980 0.45025 0.48884 0.39401 0.47652 0.45647], \n",
      "[1.00000 1.00000 1.00000 0.00000 0.00000 0.00000 1.00000 0.00000 1.00000 0.00000]), None\n",
      "in Mul backward, returning these (\n",
      "[1.00000 1.00000 1.00000 0.00000 0.00000 0.00000 1.00000 0.00000 1.00000 0.00000], \n",
      "[0.51903 0.49080 0.55094 0.47147 0.45980 0.45025 0.48884 0.39401 0.47652 0.45647])\n",
      "in chain_rule (\n",
      "[1.00000 1.00000 1.00000 0.00000 0.00000 0.00000 1.00000 0.00000 1.00000 0.00000], \n",
      "[0.51903 0.49080 0.55094 0.47147 0.45980 0.45025 0.48884 0.39401 0.47652 0.45647])\n",
      "setting diff for var None\n",
      "previous diff 0.0\n",
      "diff \n",
      "[1.00000 1.00000 1.00000 0.00000 0.00000 0.00000 1.00000 0.00000 1.00000 0.00000]\n",
      "new diff \n",
      "[1.00000 1.00000 1.00000 0.00000 0.00000 0.00000 1.00000 0.00000 1.00000 0.00000]\n",
      "======\n",
      "running backprop on var None\n",
      "getting upward diff dNone wrt. dNone\n",
      "\n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "calling Mul backward\n",
      "with grad out \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "got these saved tensors (\n",
      "[-0.48097 -0.50920 -0.44906 -0.52853 -0.54020 -0.54975 -0.51116 -0.60599 -0.52348 -0.54353], \n",
      "[0.00000 0.00000 0.00000 -1.00000 -1.00000 -1.00000 0.00000 -1.00000 0.00000 -1.00000]), None\n",
      "in Mul backward, returning these (\n",
      "[0.00000 0.00000 0.00000 -1.00000 -1.00000 -1.00000 0.00000 -1.00000 0.00000 -1.00000], \n",
      "[-0.48097 -0.50920 -0.44906 -0.52853 -0.54020 -0.54975 -0.51116 -0.60599 -0.52348 -0.54353])\n",
      "in chain_rule (\n",
      "[0.00000 0.00000 0.00000 -1.00000 -1.00000 -1.00000 0.00000 -1.00000 0.00000 -1.00000], \n",
      "[-0.48097 -0.50920 -0.44906 -0.52853 -0.54020 -0.54975 -0.51116 -0.60599 -0.52348 -0.54353])\n",
      "setting diff for var None\n",
      "previous diff 0.0\n",
      "diff \n",
      "[0.00000 0.00000 0.00000 -1.00000 -1.00000 -1.00000 0.00000 -1.00000 0.00000 -1.00000]\n",
      "new diff \n",
      "[0.00000 0.00000 0.00000 -1.00000 -1.00000 -1.00000 0.00000 -1.00000 0.00000 -1.00000]\n",
      "======\n",
      "running backprop on var None\n",
      "getting upward diff dNone wrt. dNone\n",
      "\n",
      "[0.00000 0.00000 0.00000 -1.00000 -1.00000 -1.00000 0.00000 -1.00000 0.00000 -1.00000]\n",
      "calling Add backward\n",
      "with grad out \n",
      "[0.00000 0.00000 0.00000 -1.00000 -1.00000 -1.00000 0.00000 -1.00000 0.00000 -1.00000]\n",
      "in chain_rule (\n",
      "[0.00000 0.00000 0.00000 -1.00000 -1.00000 -1.00000 0.00000 -1.00000 0.00000 -1.00000], \n",
      "[0.00000 0.00000 0.00000 -1.00000 -1.00000 -1.00000 0.00000 -1.00000 0.00000 -1.00000])\n",
      "setting diff for var None\n",
      "previous diff \n",
      "[1.00000 1.00000 1.00000 0.00000 0.00000 0.00000 1.00000 0.00000 1.00000 0.00000]\n",
      "diff \n",
      "[0.00000 0.00000 0.00000 -1.00000 -1.00000 -1.00000 0.00000 -1.00000 0.00000 -1.00000]\n",
      "new diff \n",
      "[1.00000 1.00000 1.00000 -1.00000 -1.00000 -1.00000 1.00000 -1.00000 1.00000 -1.00000]\n",
      "======\n",
      "running backprop on var None\n",
      "getting upward diff dNone wrt. dNone\n",
      "\n",
      "[1.00000 1.00000 1.00000 -1.00000 -1.00000 -1.00000 1.00000 -1.00000 1.00000 -1.00000]\n",
      "calling Sigmoid backward\n",
      "with grad out \n",
      "[1.00000 1.00000 1.00000 -1.00000 -1.00000 -1.00000 1.00000 -1.00000 1.00000 -1.00000]\n",
      "in chain_rule (\n",
      "[0.24964 0.24992 0.24740 -0.24919 -0.24838 -0.24753 0.24988 -0.23877 0.24945 -0.24811],)\n",
      "setting diff for var None\n",
      "previous diff 0.0\n",
      "diff \n",
      "[0.24964 0.24992 0.24740 -0.24919 -0.24838 -0.24753 0.24988 -0.23877 0.24945 -0.24811]\n",
      "new diff \n",
      "[0.24964 0.24992 0.24740 -0.24919 -0.24838 -0.24753 0.24988 -0.23877 0.24945 -0.24811]\n",
      "======\n",
      "running backprop on var None\n",
      "getting upward diff dNone wrt. dNone\n",
      "\n",
      "[0.24964 0.24992 0.24740 -0.24919 -0.24838 -0.24753 0.24988 -0.23877 0.24945 -0.24811]\n",
      "calling View backward\n",
      "with grad out \n",
      "[0.24964 0.24992 0.24740 -0.24919 -0.24838 -0.24753 0.24988 -0.23877 0.24945 -0.24811]\n",
      "in chain_rule (\n",
      "[\n",
      "\t[0.24964]\n",
      "\t[0.24992]\n",
      "\t[0.24740]\n",
      "\t[-0.24919]\n",
      "\t[-0.24838]\n",
      "\t[-0.24753]\n",
      "\t[0.24988]\n",
      "\t[-0.23877]\n",
      "\t[0.24945]\n",
      "\t[-0.24811]\n",
      "], \n",
      "[0.00000])\n",
      "setting diff for var None\n",
      "previous diff 0.0\n",
      "diff \n",
      "[\n",
      "\t[0.24964]\n",
      "\t[0.24992]\n",
      "\t[0.24740]\n",
      "\t[-0.24919]\n",
      "\t[-0.24838]\n",
      "\t[-0.24753]\n",
      "\t[0.24988]\n",
      "\t[-0.23877]\n",
      "\t[0.24945]\n",
      "\t[-0.24811]\n",
      "]\n",
      "new diff \n",
      "[\n",
      "\t[0.24964]\n",
      "\t[0.24992]\n",
      "\t[0.24740]\n",
      "\t[-0.24919]\n",
      "\t[-0.24838]\n",
      "\t[-0.24753]\n",
      "\t[0.24988]\n",
      "\t[-0.23877]\n",
      "\t[0.24945]\n",
      "\t[-0.24811]\n",
      "]\n",
      "======\n",
      "running backprop on var None\n",
      "getting upward diff dNone wrt. dNone\n",
      "\n",
      "[\n",
      "\t[0.24964]\n",
      "\t[0.24992]\n",
      "\t[0.24740]\n",
      "\t[-0.24919]\n",
      "\t[-0.24838]\n",
      "\t[-0.24753]\n",
      "\t[0.24988]\n",
      "\t[-0.23877]\n",
      "\t[0.24945]\n",
      "\t[-0.24811]\n",
      "]\n",
      "calling Add backward\n",
      "with grad out \n",
      "[\n",
      "\t[0.24964]\n",
      "\t[0.24992]\n",
      "\t[0.24740]\n",
      "\t[-0.24919]\n",
      "\t[-0.24838]\n",
      "\t[-0.24753]\n",
      "\t[0.24988]\n",
      "\t[-0.23877]\n",
      "\t[0.24945]\n",
      "\t[-0.24811]\n",
      "]\n",
      "in chain_rule (\n",
      "[\n",
      "\t[0.24964]\n",
      "\t[0.24992]\n",
      "\t[0.24740]\n",
      "\t[-0.24919]\n",
      "\t[-0.24838]\n",
      "\t[-0.24753]\n",
      "\t[0.24988]\n",
      "\t[-0.23877]\n",
      "\t[0.24945]\n",
      "\t[-0.24811]\n",
      "], \n",
      "[\n",
      "\t[0.24964]\n",
      "\t[0.24992]\n",
      "\t[0.24740]\n",
      "\t[-0.24919]\n",
      "\t[-0.24838]\n",
      "\t[-0.24753]\n",
      "\t[0.24988]\n",
      "\t[-0.23877]\n",
      "\t[0.24945]\n",
      "\t[-0.24811]\n",
      "])\n",
      "setting diff for var None\n",
      "previous diff 0.0\n",
      "diff \n",
      "[\n",
      "\t[0.24964]\n",
      "\t[0.24992]\n",
      "\t[0.24740]\n",
      "\t[-0.24919]\n",
      "\t[-0.24838]\n",
      "\t[-0.24753]\n",
      "\t[0.24988]\n",
      "\t[-0.23877]\n",
      "\t[0.24945]\n",
      "\t[-0.24811]\n",
      "]\n",
      "new diff \n",
      "[\n",
      "\t[0.24964]\n",
      "\t[0.24992]\n",
      "\t[0.24740]\n",
      "\t[-0.24919]\n",
      "\t[-0.24838]\n",
      "\t[-0.24753]\n",
      "\t[0.24988]\n",
      "\t[-0.23877]\n",
      "\t[0.24945]\n",
      "\t[-0.24811]\n",
      "]\n",
      "setting diff for var None\n",
      "previous diff 0.0\n",
      "diff \n",
      "[0.01431]\n",
      "new diff \n",
      "[0.01431]\n",
      "======\n",
      "running backprop on var None\n",
      "getting upward diff dNone wrt. dNone\n",
      "\n",
      "[\n",
      "\t[0.24964]\n",
      "\t[0.24992]\n",
      "\t[0.24740]\n",
      "\t[-0.24919]\n",
      "\t[-0.24838]\n",
      "\t[-0.24753]\n",
      "\t[0.24988]\n",
      "\t[-0.23877]\n",
      "\t[0.24945]\n",
      "\t[-0.24811]\n",
      "]\n",
      "calling View backward\n",
      "with grad out \n",
      "[\n",
      "\t[0.24964]\n",
      "\t[0.24992]\n",
      "\t[0.24740]\n",
      "\t[-0.24919]\n",
      "\t[-0.24838]\n",
      "\t[-0.24753]\n",
      "\t[0.24988]\n",
      "\t[-0.23877]\n",
      "\t[0.24945]\n",
      "\t[-0.24811]\n",
      "]\n",
      "in chain_rule (\n",
      "[\n",
      "\t[\n",
      "\t\t[0.24964]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24992]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24740]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24919]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24838]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24753]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24988]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.23877]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24945]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24811]\n",
      "\t]\n",
      "], \n",
      "[0.00000])\n",
      "setting diff for var None\n",
      "previous diff 0.0\n",
      "diff \n",
      "[\n",
      "\t[\n",
      "\t\t[0.24964]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24992]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24740]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24919]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24838]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24753]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24988]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.23877]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24945]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24811]\n",
      "\t]\n",
      "]\n",
      "new diff \n",
      "[\n",
      "\t[\n",
      "\t\t[0.24964]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24992]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24740]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24919]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24838]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24753]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24988]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.23877]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24945]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24811]\n",
      "\t]\n",
      "]\n",
      "======\n",
      "======\n",
      "running backprop on var None\n",
      "getting upward diff dNone wrt. dNone\n",
      "\n",
      "[\n",
      "\t[\n",
      "\t\t[0.24964]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24992]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24740]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24919]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24838]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24753]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24988]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.23877]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24945]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24811]\n",
      "\t]\n",
      "]\n",
      "calling Sum backward\n",
      "with grad out \n",
      "[\n",
      "\t[\n",
      "\t\t[0.24964]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24992]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24740]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24919]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24838]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24753]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24988]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.23877]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24945]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24811]\n",
      "\t]\n",
      "]\n",
      "in chain_rule (\n",
      "[\n",
      "\t[\n",
      "\t\t[0.24964]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24992]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24740]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24919]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24838]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24753]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24988]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.23877]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24945]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24811]\n",
      "\t]\n",
      "], \n",
      "[0.00000])\n",
      "setting diff for var None\n",
      "previous diff 0.0\n",
      "diff \n",
      "[\n",
      "\t[\n",
      "\t\t[0.24964]\n",
      "\t\t[0.24964]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24992]\n",
      "\t\t[0.24992]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24740]\n",
      "\t\t[0.24740]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24919]\n",
      "\t\t[-0.24919]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24838]\n",
      "\t\t[-0.24838]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24753]\n",
      "\t\t[-0.24753]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24988]\n",
      "\t\t[0.24988]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.23877]\n",
      "\t\t[-0.23877]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24945]\n",
      "\t\t[0.24945]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24811]\n",
      "\t\t[-0.24811]\n",
      "\t]\n",
      "]\n",
      "new diff \n",
      "[\n",
      "\t[\n",
      "\t\t[0.24964]\n",
      "\t\t[0.24964]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24992]\n",
      "\t\t[0.24992]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24740]\n",
      "\t\t[0.24740]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24919]\n",
      "\t\t[-0.24919]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24838]\n",
      "\t\t[-0.24838]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24753]\n",
      "\t\t[-0.24753]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24988]\n",
      "\t\t[0.24988]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.23877]\n",
      "\t\t[-0.23877]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24945]\n",
      "\t\t[0.24945]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24811]\n",
      "\t\t[-0.24811]\n",
      "\t]\n",
      "]\n",
      "======\n",
      "running backprop on var None\n",
      "getting upward diff dNone wrt. dNone\n",
      "\n",
      "[\n",
      "\t[\n",
      "\t\t[0.24964]\n",
      "\t\t[0.24964]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24992]\n",
      "\t\t[0.24992]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24740]\n",
      "\t\t[0.24740]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24919]\n",
      "\t\t[-0.24919]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24838]\n",
      "\t\t[-0.24838]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24753]\n",
      "\t\t[-0.24753]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24988]\n",
      "\t\t[0.24988]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.23877]\n",
      "\t\t[-0.23877]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24945]\n",
      "\t\t[0.24945]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24811]\n",
      "\t\t[-0.24811]\n",
      "\t]\n",
      "]\n",
      "calling Mul backward\n",
      "with grad out \n",
      "[\n",
      "\t[\n",
      "\t\t[0.24964]\n",
      "\t\t[0.24964]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24992]\n",
      "\t\t[0.24992]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24740]\n",
      "\t\t[0.24740]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24919]\n",
      "\t\t[-0.24919]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24838]\n",
      "\t\t[-0.24838]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24753]\n",
      "\t\t[-0.24753]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24988]\n",
      "\t\t[0.24988]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.23877]\n",
      "\t\t[-0.23877]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24945]\n",
      "\t\t[0.24945]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24811]\n",
      "\t\t[-0.24811]\n",
      "\t]\n",
      "]\n",
      "got these saved tensors (\n",
      "[\n",
      "\t[\n",
      "\t\t[0.77444]\n",
      "\t\t[0.84215]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.76528]\n",
      "\t\t[0.69929]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.15482]\n",
      "\t\t[0.62544]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.85428]\n",
      "\t\t[0.65871]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.13102]\n",
      "\t\t[0.16660]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.97967]\n",
      "\t\t[0.63030]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.88772]\n",
      "\t\t[0.76341]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.85397]\n",
      "\t\t[0.27406]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.69646]\n",
      "\t\t[0.58840]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.17008]\n",
      "\t\t[0.17382]\n",
      "\t]\n",
      "], \n",
      "[\n",
      "\t[\n",
      "\t\t[-0.49478]\n",
      "\t\t[0.82254]\n",
      "\t]\n",
      "]), None\n",
      "in Mul backward, returning these (\n",
      "[\n",
      "\t[\n",
      "\t\t[-0.12352]\n",
      "\t\t[0.20534]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.12365]\n",
      "\t\t[0.20557]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.12241]\n",
      "\t\t[0.20350]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.12329]\n",
      "\t\t[-0.20497]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.12290]\n",
      "\t\t[-0.20431]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.12247]\n",
      "\t\t[-0.20360]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.12363]\n",
      "\t\t[0.20553]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.11814]\n",
      "\t\t[-0.19640]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.12342]\n",
      "\t\t[0.20518]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.12276]\n",
      "\t\t[-0.20408]\n",
      "\t]\n",
      "], \n",
      "[\n",
      "\t[\n",
      "\t\t[0.19333]\n",
      "\t\t[0.21023]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.19125]\n",
      "\t\t[0.17476]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.03830]\n",
      "\t\t[0.15474]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.21287]\n",
      "\t\t[-0.16414]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.03254]\n",
      "\t\t[-0.04138]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24249]\n",
      "\t\t[-0.15601]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.22182]\n",
      "\t\t[0.19076]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.20390]\n",
      "\t\t[-0.06544]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.17373]\n",
      "\t\t[0.14678]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.04220]\n",
      "\t\t[-0.04313]\n",
      "\t]\n",
      "])\n",
      "in chain_rule (\n",
      "[\n",
      "\t[\n",
      "\t\t[-0.12352]\n",
      "\t\t[0.20534]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.12365]\n",
      "\t\t[0.20557]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.12241]\n",
      "\t\t[0.20350]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.12329]\n",
      "\t\t[-0.20497]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.12290]\n",
      "\t\t[-0.20431]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.12247]\n",
      "\t\t[-0.20360]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.12363]\n",
      "\t\t[0.20553]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.11814]\n",
      "\t\t[-0.19640]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.12342]\n",
      "\t\t[0.20518]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.12276]\n",
      "\t\t[-0.20408]\n",
      "\t]\n",
      "], \n",
      "[\n",
      "\t[\n",
      "\t\t[0.19333]\n",
      "\t\t[0.21023]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.19125]\n",
      "\t\t[0.17476]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.03830]\n",
      "\t\t[0.15474]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.21287]\n",
      "\t\t[-0.16414]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.03254]\n",
      "\t\t[-0.04138]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24249]\n",
      "\t\t[-0.15601]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.22182]\n",
      "\t\t[0.19076]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.20390]\n",
      "\t\t[-0.06544]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.17373]\n",
      "\t\t[0.14678]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.04220]\n",
      "\t\t[-0.04313]\n",
      "\t]\n",
      "])\n",
      "setting diff for var None\n",
      "previous diff 0.0\n",
      "diff \n",
      "[\n",
      "\t[\n",
      "\t\t[0.08443]\n",
      "\t\t[0.40717]\n",
      "\t]\n",
      "]\n",
      "new diff \n",
      "[\n",
      "\t[\n",
      "\t\t[0.08443]\n",
      "\t\t[0.40717]\n",
      "\t]\n",
      "]\n",
      "======\n",
      "running backprop on var None\n",
      "getting upward diff dNone wrt. dNone\n",
      "\n",
      "[\n",
      "\t[\n",
      "\t\t[0.08443]\n",
      "\t\t[0.40717]\n",
      "\t]\n",
      "]\n",
      "calling View backward\n",
      "with grad out \n",
      "[\n",
      "\t[\n",
      "\t\t[0.08443]\n",
      "\t\t[0.40717]\n",
      "\t]\n",
      "]\n",
      "in chain_rule (\n",
      "[\n",
      "\t[0.08443]\n",
      "\t[0.40717]\n",
      "], \n",
      "[0.00000])\n",
      "setting diff for var None\n",
      "previous diff 0.0\n",
      "diff \n",
      "[\n",
      "\t[0.08443]\n",
      "\t[0.40717]\n",
      "]\n",
      "new diff \n",
      "[\n",
      "\t[0.08443]\n",
      "\t[0.40717]\n",
      "]\n",
      "======\n",
      "======\n",
      "assigning derivatives\n",
      "None: \n",
      "[1.00000]\n",
      "None: \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "None: \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "None: \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "None: \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "None: \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "None: \n",
      "[1.00000 1.00000 1.00000 -1.00000 -1.00000 -1.00000 1.00000 -1.00000 1.00000 -1.00000]\n",
      "None: \n",
      "[0.00000 0.00000 0.00000 -1.00000 -1.00000 -1.00000 0.00000 -1.00000 0.00000 -1.00000]\n",
      "None: \n",
      "[0.24964 0.24992 0.24740 -0.24919 -0.24838 -0.24753 0.24988 -0.23877 0.24945 -0.24811]\n",
      "None: \n",
      "[\n",
      "\t[0.24964]\n",
      "\t[0.24992]\n",
      "\t[0.24740]\n",
      "\t[-0.24919]\n",
      "\t[-0.24838]\n",
      "\t[-0.24753]\n",
      "\t[0.24988]\n",
      "\t[-0.23877]\n",
      "\t[0.24945]\n",
      "\t[-0.24811]\n",
      "]\n",
      "None: \n",
      "[\n",
      "\t[0.24964]\n",
      "\t[0.24992]\n",
      "\t[0.24740]\n",
      "\t[-0.24919]\n",
      "\t[-0.24838]\n",
      "\t[-0.24753]\n",
      "\t[0.24988]\n",
      "\t[-0.23877]\n",
      "\t[0.24945]\n",
      "\t[-0.24811]\n",
      "]\n",
      "None: \n",
      "[0.01431]\n",
      "None: \n",
      "[\n",
      "\t[\n",
      "\t\t[0.24964]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24992]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24740]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24919]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24838]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24753]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24988]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.23877]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24945]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24811]\n",
      "\t]\n",
      "]\n",
      "None: \n",
      "[\n",
      "\t[\n",
      "\t\t[0.24964]\n",
      "\t\t[0.24964]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24992]\n",
      "\t\t[0.24992]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24740]\n",
      "\t\t[0.24740]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24919]\n",
      "\t\t[-0.24919]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24838]\n",
      "\t\t[-0.24838]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24753]\n",
      "\t\t[-0.24753]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24988]\n",
      "\t\t[0.24988]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.23877]\n",
      "\t\t[-0.23877]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[0.24945]\n",
      "\t\t[0.24945]\n",
      "\t]\n",
      "\t[\n",
      "\t\t[-0.24811]\n",
      "\t\t[-0.24811]\n",
      "\t]\n",
      "]\n",
      "None: \n",
      "[\n",
      "\t[\n",
      "\t\t[0.08443]\n",
      "\t\t[0.40717]\n",
      "\t]\n",
      "]\n",
      "None: \n",
      "[\n",
      "\t[0.08443]\n",
      "\t[0.40717]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "input_dim, output_dim = 2, 1\n",
    "weights = LinearTensorLayer._initialise_parameter(input_dim, output_dim).value\n",
    "bias = LinearTensorLayer._initialise_parameter(output_dim).value\n",
    "\n",
    "# Generate some input data\n",
    "n_samples = 10\n",
    "inputs = tf.rand((n_samples, input_dim))\n",
    "targets = tf.tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 0])\n",
    "\n",
    "# Forward\n",
    "inputs = inputs.view(*inputs.shape, 1)\n",
    "_weights = weights.view(1, *weights.shape)\n",
    "\n",
    "out = (inputs * _weights).sum(dim=1)\n",
    "predictions = out.view(inputs.shape[0], bias.size) + bias\n",
    "predictions = predictions.view(targets.size).sigmoid()\n",
    "\n",
    "predictions_ = (predictions * targets) + (predictions - 1.0) * (targets - 1.0)\n",
    "predictions_sum = predictions_.sum()\n",
    "predictions_sum.backward()\n",
    "\n",
    "# Compute loss\n",
    "# probas = (predictions * targets) + (predictions - 1.0) * (targets - 1.0)\n",
    "# loss = ((-probas.log()) / targets.size).sum()\n",
    "# loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loss\n",
    "targets = tf.tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 0])\n",
    "targets.name = \"targets\"\n",
    "\n",
    "out = tf.rand((10, ), requires_grad=True)\n",
    "out.name = \"out\"\n",
    "\n",
    "predictions = out.sigmoid()\n",
    "predictions.name = \"predictions\"\n",
    "\n",
    "predictions_x_targets = predictions * targets\n",
    "predictions_x_targets.name = \"predictions_x_targets\"\n",
    "\n",
    "predictions_m_1 = (predictions - 1.0)\n",
    "predictions_m_1.name = \"predictions_m_1\"\n",
    "\n",
    "targets_m_1 = (targets - 1.0)\n",
    "targets_m_1.name = \"targets_m_1\"\n",
    "\n",
    "pred_m_1_x_target_m_1 = predictions_m_1 * targets_m_1\n",
    "pred_m_1_x_target_m_1.name = \"pred_m_1_x_target_m_1\"\n",
    "\n",
    "predictions_ = predictions_x_targets + pred_m_1_x_target_m_1\n",
    "predictions_.name = \"predictions_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions_\n",
      "[]\n",
      "['predictions_']\n",
      "predictions_x_targets\n",
      "['predictions_']\n",
      "['predictions_', 'predictions_x_targets']\n",
      "pred_m_1_x_target_m_1\n",
      "['predictions_', 'predictions_x_targets']\n",
      "['predictions_', 'predictions_x_targets', 'pred_m_1_x_target_m_1']\n",
      "predictions\n",
      "['predictions_', 'predictions_x_targets', 'pred_m_1_x_target_m_1']\n",
      "['predictions_', 'predictions_x_targets', 'pred_m_1_x_target_m_1', 'predictions']\n",
      "predictions_m_1\n",
      "['predictions_', 'predictions_x_targets', 'pred_m_1_x_target_m_1', 'predictions']\n",
      "['predictions_', 'predictions_x_targets', 'pred_m_1_x_target_m_1', 'predictions', 'predictions_m_1']\n",
      "out\n",
      "['predictions_', 'predictions_x_targets', 'pred_m_1_x_target_m_1', 'predictions', 'predictions_m_1']\n",
      "['predictions_', 'predictions_x_targets', 'pred_m_1_x_target_m_1', 'predictions', 'predictions_m_1', 'out']\n",
      "predictions\n",
      "['predictions_', 'predictions_x_targets', 'pred_m_1_x_target_m_1', 'predictions', 'predictions_m_1', 'out']\n",
      "have seen predictions before\n",
      "removing predictions\n",
      "ids ....\n",
      "['predictions_', 'predictions_x_targets', 'pred_m_1_x_target_m_1', 'predictions_m_1', 'out', 'predictions']\n",
      "out\n",
      "['predictions_', 'predictions_x_targets', 'pred_m_1_x_target_m_1', 'predictions_m_1', 'out', 'predictions']\n",
      "have seen out before\n",
      "removing out\n",
      "ids ....\n",
      "['predictions_', 'predictions_x_targets', 'pred_m_1_x_target_m_1', 'predictions_m_1', 'predictions', 'out']\n"
     ]
    }
   ],
   "source": [
    "diff_chain = topological_sort(predictions_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[t.name for t in diff_chain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[]\n",
      "[None]\n",
      "None\n",
      "[None]\n",
      "[None, None]\n",
      "None\n",
      "[None, None]\n",
      "[None, None, None]\n",
      "predictions_\n",
      "[None, None, None]\n",
      "[None, None, None, 'predictions_']\n",
      "predictions_x_targets\n",
      "[None, None, None, 'predictions_']\n",
      "[None, None, None, 'predictions_', 'predictions_x_targets']\n",
      "pred_m_1_x_target_m_1\n",
      "[None, None, None, 'predictions_', 'predictions_x_targets']\n",
      "[None, None, None, 'predictions_', 'predictions_x_targets', 'pred_m_1_x_target_m_1']\n",
      "predictions\n",
      "[None, None, None, 'predictions_', 'predictions_x_targets', 'pred_m_1_x_target_m_1']\n",
      "[None, None, None, 'predictions_', 'predictions_x_targets', 'pred_m_1_x_target_m_1', 'predictions']\n",
      "predictions_m_1\n",
      "[None, None, None, 'predictions_', 'predictions_x_targets', 'pred_m_1_x_target_m_1', 'predictions']\n",
      "[None, None, None, 'predictions_', 'predictions_x_targets', 'pred_m_1_x_target_m_1', 'predictions', 'predictions_m_1']\n",
      "out\n",
      "[None, None, None, 'predictions_', 'predictions_x_targets', 'pred_m_1_x_target_m_1', 'predictions', 'predictions_m_1']\n",
      "[None, None, None, 'predictions_', 'predictions_x_targets', 'pred_m_1_x_target_m_1', 'predictions', 'predictions_m_1', 'out']\n",
      "predictions\n",
      "[None, None, None, 'predictions_', 'predictions_x_targets', 'pred_m_1_x_target_m_1', 'predictions', 'predictions_m_1', 'out']\n",
      "have seen predictions before\n",
      "removing predictions\n",
      "ids ....\n",
      "[None, None, None, 'predictions_', 'predictions_x_targets', 'pred_m_1_x_target_m_1', 'predictions_m_1', 'out', 'predictions']\n",
      "out\n",
      "[None, None, None, 'predictions_', 'predictions_x_targets', 'pred_m_1_x_target_m_1', 'predictions_m_1', 'out', 'predictions']\n",
      "have seen out before\n",
      "removing out\n",
      "ids ....\n",
      "[None, None, None, 'predictions_', 'predictions_x_targets', 'pred_m_1_x_target_m_1', 'predictions_m_1', 'predictions', 'out']\n",
      "running backprop on var None\n",
      "getting upward diff None\n",
      "\n",
      "[1.00000]\n",
      "calling Sum backward\n",
      "with grad out \n",
      "[1.00000]\n",
      "in chain_rule (\n",
      "[1.00000], \n",
      "[0.00000])\n",
      "setting diff for var None\n",
      "previous diff 0.0\n",
      "diff \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "new diff \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "======\n",
      "running backprop on var None\n",
      "getting upward diff dNone wrt. dNone\n",
      "\n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "calling View backward\n",
      "with grad out \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "in chain_rule (\n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000], \n",
      "[0.00000])\n",
      "setting diff for var None\n",
      "previous diff 0.0\n",
      "diff \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "new diff \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "======\n",
      "running backprop on var None\n",
      "getting upward diff dNone wrt. dNone\n",
      "\n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "calling Copy backward\n",
      "with grad out \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "in chain_rule (\n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000],)\n",
      "setting diff for var predictions_\n",
      "previous diff 0.0\n",
      "diff \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "new diff \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "======\n",
      "running backprop on var predictions_\n",
      "getting upward diff dNone wrt. dpredictions_\n",
      "\n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "calling Add backward\n",
      "with grad out \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "in chain_rule (\n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000], \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000])\n",
      "setting diff for var predictions_x_targets\n",
      "previous diff 0.0\n",
      "diff \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "new diff \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "setting diff for var pred_m_1_x_target_m_1\n",
      "previous diff 0.0\n",
      "diff \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "new diff \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "======\n",
      "running backprop on var predictions_x_targets\n",
      "getting upward diff dpredictions_ wrt. dpredictions_x_targets\n",
      "\n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "calling Mul backward\n",
      "with grad out \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "got these saved tensors (\n",
      "[0.52620 0.62720 0.50897 0.70822 0.57785 0.57537 0.61607 0.50944 0.68498 0.67842], \n",
      "[1.00000 1.00000 1.00000 0.00000 0.00000 0.00000 1.00000 0.00000 1.00000 0.00000]), None\n",
      "in Mul backward, returning these (\n",
      "[1.00000 1.00000 1.00000 0.00000 0.00000 0.00000 1.00000 0.00000 1.00000 0.00000], \n",
      "[0.52620 0.62720 0.50897 0.70822 0.57785 0.57537 0.61607 0.50944 0.68498 0.67842])\n",
      "in chain_rule (\n",
      "[1.00000 1.00000 1.00000 0.00000 0.00000 0.00000 1.00000 0.00000 1.00000 0.00000], \n",
      "[0.52620 0.62720 0.50897 0.70822 0.57785 0.57537 0.61607 0.50944 0.68498 0.67842])\n",
      "setting diff for var predictions\n",
      "previous diff 0.0\n",
      "diff \n",
      "[1.00000 1.00000 1.00000 0.00000 0.00000 0.00000 1.00000 0.00000 1.00000 0.00000]\n",
      "new diff \n",
      "[1.00000 1.00000 1.00000 0.00000 0.00000 0.00000 1.00000 0.00000 1.00000 0.00000]\n",
      "======\n",
      "running backprop on var pred_m_1_x_target_m_1\n",
      "getting upward diff dpredictions_ wrt. dpred_m_1_x_target_m_1\n",
      "\n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "calling Mul backward\n",
      "with grad out \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "got these saved tensors (\n",
      "[-0.47380 -0.37280 -0.49103 -0.29178 -0.42215 -0.42463 -0.38393 -0.49056 -0.31502 -0.32158], \n",
      "[0.00000 0.00000 0.00000 -1.00000 -1.00000 -1.00000 0.00000 -1.00000 0.00000 -1.00000]), None\n",
      "in Mul backward, returning these (\n",
      "[0.00000 0.00000 0.00000 -1.00000 -1.00000 -1.00000 0.00000 -1.00000 0.00000 -1.00000], \n",
      "[-0.47380 -0.37280 -0.49103 -0.29178 -0.42215 -0.42463 -0.38393 -0.49056 -0.31502 -0.32158])\n",
      "in chain_rule (\n",
      "[0.00000 0.00000 0.00000 -1.00000 -1.00000 -1.00000 0.00000 -1.00000 0.00000 -1.00000], \n",
      "[-0.47380 -0.37280 -0.49103 -0.29178 -0.42215 -0.42463 -0.38393 -0.49056 -0.31502 -0.32158])\n",
      "setting diff for var predictions_m_1\n",
      "previous diff 0.0\n",
      "diff \n",
      "[0.00000 0.00000 0.00000 -1.00000 -1.00000 -1.00000 0.00000 -1.00000 0.00000 -1.00000]\n",
      "new diff \n",
      "[0.00000 0.00000 0.00000 -1.00000 -1.00000 -1.00000 0.00000 -1.00000 0.00000 -1.00000]\n",
      "======\n",
      "running backprop on var predictions_m_1\n",
      "getting upward diff dpred_m_1_x_target_m_1 wrt. dpredictions_m_1\n",
      "\n",
      "[0.00000 0.00000 0.00000 -1.00000 -1.00000 -1.00000 0.00000 -1.00000 0.00000 -1.00000]\n",
      "calling Add backward\n",
      "with grad out \n",
      "[0.00000 0.00000 0.00000 -1.00000 -1.00000 -1.00000 0.00000 -1.00000 0.00000 -1.00000]\n",
      "in chain_rule (\n",
      "[0.00000 0.00000 0.00000 -1.00000 -1.00000 -1.00000 0.00000 -1.00000 0.00000 -1.00000], \n",
      "[0.00000 0.00000 0.00000 -1.00000 -1.00000 -1.00000 0.00000 -1.00000 0.00000 -1.00000])\n",
      "setting diff for var predictions\n",
      "previous diff \n",
      "[1.00000 1.00000 1.00000 0.00000 0.00000 0.00000 1.00000 0.00000 1.00000 0.00000]\n",
      "diff \n",
      "[0.00000 0.00000 0.00000 -1.00000 -1.00000 -1.00000 0.00000 -1.00000 0.00000 -1.00000]\n",
      "new diff \n",
      "[1.00000 1.00000 1.00000 -1.00000 -1.00000 -1.00000 1.00000 -1.00000 1.00000 -1.00000]\n",
      "======\n",
      "running backprop on var predictions\n",
      "getting upward diff dpredictions_m_1 wrt. dpredictions\n",
      "\n",
      "[1.00000 1.00000 1.00000 -1.00000 -1.00000 -1.00000 1.00000 -1.00000 1.00000 -1.00000]\n",
      "calling Sigmoid backward\n",
      "with grad out \n",
      "[1.00000 1.00000 1.00000 -1.00000 -1.00000 -1.00000 1.00000 -1.00000 1.00000 -1.00000]\n",
      "in chain_rule (\n",
      "[0.24931 0.23382 0.24992 -0.20664 -0.24394 -0.24432 0.23653 -0.24991 0.21578 -0.21817],)\n",
      "setting diff for var out\n",
      "previous diff 0.0\n",
      "diff \n",
      "[0.24931 0.23382 0.24992 -0.20664 -0.24394 -0.24432 0.23653 -0.24991 0.21578 -0.21817]\n",
      "new diff \n",
      "[0.24931 0.23382 0.24992 -0.20664 -0.24394 -0.24432 0.23653 -0.24991 0.21578 -0.21817]\n",
      "======\n",
      "======\n",
      "assigning derivatives\n",
      "None: \n",
      "[1.00000]\n",
      "None: \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "None: \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "predictions_: \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "predictions_x_targets: \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "pred_m_1_x_target_m_1: \n",
      "[1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000]\n",
      "predictions: \n",
      "[1.00000 1.00000 1.00000 -1.00000 -1.00000 -1.00000 1.00000 -1.00000 1.00000 -1.00000]\n",
      "predictions_m_1: \n",
      "[0.00000 0.00000 0.00000 -1.00000 -1.00000 -1.00000 0.00000 -1.00000 0.00000 -1.00000]\n",
      "out: \n",
      "[0.24931 0.23382 0.24992 -0.20664 -0.24394 -0.24432 0.23653 -0.24991 0.21578 -0.21817]\n"
     ]
    }
   ],
   "source": [
    "predictions_.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out grad \n",
      "[0.24931 0.23382 0.24992 -0.20664 -0.24394 -0.24432 0.23653 -0.24991 0.21578 -0.21817]\n",
      "predictions grad \n",
      "[1.00000 1.00000 1.00000 -1.00000 -1.00000 -1.00000 1.00000 -1.00000 1.00000 -1.00000]\n"
     ]
    }
   ],
   "source": [
    "print(f\"out grad {out.grad}\")\n",
    "print(f\"predictions grad {predictions.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare to torch\n",
    "torch_targets = torch.tensor(targets.data.storage)\n",
    "torch_out = torch.tensor(out.data.storage, requires_grad=True)\n",
    "torch_predictions = torch_out.sigmoid()\n",
    "torch_predictions.retain_grad = True\n",
    "\n",
    "torch_predictions_ = (torch_predictions * torch_targets) + (torch_predictions - 1.0) * (torch_targets - 1.0)\n",
    "torch_predictions_.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out grad tensor([ 0.2493,  0.2338,  0.2499, -0.2066, -0.2439, -0.2443,  0.2365, -0.2499,\n",
      "         0.2158, -0.2182], dtype=torch.float64)\n",
      "predictions grad \n",
      "[1.00000 1.00000 1.00000 -1.00000 -1.00000 -1.00000 1.00000 -1.00000 1.00000 -1.00000]\n"
     ]
    }
   ],
   "source": [
    "print(f\"out grad {torch_out.grad}\")\n",
    "print(f\"predictions grad {predictions.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loss without sigmoid\n",
    "predictions = tf.rand((10, ), requires_grad=True)\n",
    "targets = tf.tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 0])\n",
    "\n",
    "predictions_ = (predictions * targets) + (predictions - 1.0) * (targets - 1.0)\n",
    "predictions_.sum().backward()\n",
    "\n",
    "predictions.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare to torch\n",
    "torch_predictions = torch.tensor(predictions.data.storage, requires_grad=True)\n",
    "torch_targets = torch.tensor(targets.data.storage)\n",
    "\n",
    "torch_predictions_ = (torch_predictions * torch_targets) + (torch_predictions - 1.0) * (torch_targets - 1.0)\n",
    "torch_predictions_.sum().backward()\n",
    "\n",
    "torch_predictions.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('minitorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b3eb730fa1a7143dca486765832757be3254ffce0a0c66fb50fd2d4e1bf0a07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
